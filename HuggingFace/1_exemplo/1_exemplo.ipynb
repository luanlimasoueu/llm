{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 629/629 [00:00<00:00, 402kB/s]\n",
      "c:\\Users\\Luan Lima\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Luan Lima\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 268M/268M [04:47<00:00, 933kB/s] \n",
      "tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.67MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9516069293022156}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "res = classifier(\"I've been waiting for a HuggingFace course my whole life\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 333kB/s]\n",
      "c:\\Users\\Luan Lima\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Luan Lima\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 548M/548M [09:35<00:00, 953kB/s] \n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 62.0kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<?, ?B/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.07MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 869kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:01<00:00, 1.05MB/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"}, {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"}, {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"}, {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"}, {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "res = generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
